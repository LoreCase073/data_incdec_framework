{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports to work with...\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from cl_framework.continual_learning.metrics.metric_evaluator_incdec import MetricEvaluatorIncDec\n",
    "from cl_framework.utilities.matrix_logger import IncDecLogger\n",
    "from torchmetrics import Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = ['../runs_trainings/no_freeze/multilabel/weighted',\n",
    "                '../runs_trainings/no_freeze/decremental_multilabel/weighted',         \n",
    "                '../runs_trainings/no_freeze/incremental_decremental_multilabel/weighted',\n",
    "                '../runs_trainings/no_freeze/joint_incremental_multilabel/weighted',\n",
    "                '../runs_trainings/no_freeze/joint_incremental_restored_multilabel/weighted/reset',\n",
    "\n",
    "                '../runs_trainings/freeze_backbone/joint_incremental_restored_multilabel/weighted/reset',\n",
    "                '../runs_trainings/freeze_backbone/joint_incremental_multilabel/weighted/new',\n",
    "                '../runs_trainings/freeze_backbone/incremental_decremental_multilabel/weighted/new',\n",
    "                '../runs_trainings/freeze_backbone/decremental_multilabel/weighted/new',\n",
    "                '../runs_trainings/freeze_backbone/baseline_multilabel/weighted/new',\n",
    "\n",
    "                '../runs_trainings/fd/incremental_decremental/lambda_1',\n",
    "                '../runs_trainings/fd/incremental_decremental/lambda_01',\n",
    "                '../runs_trainings/fd/incremental_decremental/lambda_10',\n",
    "                '../runs_trainings/fd/decremental/lambda_01',\n",
    "                '../runs_trainings/fd/joint_incremental/lambda_01',\n",
    "                '../runs_trainings/fd/baseline/lambda_01',\n",
    "\n",
    "\n",
    "                '../runs_trainings/lwf/incremental_decremental/lambda_10/temp_1',\n",
    "                '../runs_trainings/lwf/incremental_decremental/lambda_2/temp_1',\n",
    "                '../runs_trainings/lwf/incremental_decremental/lambda_1/temp_1',\n",
    "                '../runs_trainings/lwf/baseline/lambda_1',\n",
    "                '../runs_trainings/lwf/decremental/lambda_1',\n",
    "                '../runs_trainings/lwf/joint_incremental/lambda_1',\n",
    "\n",
    "\n",
    "                ]\n",
    "seeds = [0,1,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "forg_multidim_file_names = ['forg_accuracy_per_subcategory.out','forg_ap_per_subcategory.out',\n",
    "                       'forg_recall_per_subcategory.out','forg_precision_per_subcategory.out']\n",
    "\n",
    "data_dict = {\n",
    "    'food': [\n",
    "        'eating burger', 'eating cake', 'eating carrots', 'eating chips', 'eating doughnuts',\n",
    "        'eating hotdog', 'eating ice cream', 'eating spaghetti', 'eating watermelon',\n",
    "        'sucking lolly', 'tasting beer', 'tasting food', 'tasting wine', 'sipping cup'\n",
    "    ],\n",
    "    'phone': [\n",
    "        'texting', 'talking on cell phone', 'looking at phone'\n",
    "    ],\n",
    "    'smoking': [\n",
    "        'smoking', 'smoking hookah', 'smoking pipe'\n",
    "    ],\n",
    "    'fatigue': [\n",
    "        'sleeping', 'yawning', 'headbanging', 'headbutting', 'shaking head'\n",
    "    ],\n",
    "    'selfcare': [\n",
    "        'scrubbing face', 'putting in contact lenses', 'putting on eyeliner', 'putting on foundation',\n",
    "        'putting on lipstick', 'putting on mascara', 'brushing hair', 'brushing teeth', 'braiding hair',\n",
    "        'combing hair', 'dyeing eyebrows', 'dyeing hair'\n",
    "    ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res_path in results_path:\n",
    "    statistics_save_path = os.path.join(res_path,'statistics')\n",
    "    if not os.path.exists(statistics_save_path):\n",
    "        os.mkdir(statistics_save_path)\n",
    "    mean_std_over_tasks_path = os.path.join(statistics_save_path,'mean_std_forg_subcat_over_tasks')\n",
    "    if not os.path.exists(mean_std_over_tasks_path):\n",
    "        os.mkdir(mean_std_over_tasks_path)\n",
    "    mean_over_tasks_path = os.path.join(statistics_save_path,'mean_forg_subcat_over_tasks')\n",
    "    if not os.path.exists(mean_over_tasks_path):\n",
    "        os.mkdir(mean_over_tasks_path)\n",
    "    std_over_tasks_path = os.path.join(statistics_save_path,'std_forg_subcat_over_tasks')\n",
    "    if not os.path.exists(std_over_tasks_path):\n",
    "        os.mkdir(std_over_tasks_path)\n",
    "    \n",
    "    data = {}\n",
    "    subcategories_names = None\n",
    "    for file_name in forg_multidim_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data[1:])\n",
    "                f = open(file_path)\n",
    "                header = f.readline()\n",
    "                subcategories_names = header[2:].replace('\\n','').split(',')\n",
    "    \n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    mean_over_tasks = {}\n",
    "    for file_name in forg_multidim_file_names:\n",
    "        # this is done because i have the ap in 0,1 range, want to be in percentage\n",
    "        if not ((file_name == 'mean_ap.out') or (file_name == 'map_weighted.out') or (file_name == 'forg_mean_ap.out') or (file_name == 'forg_acc.out') or (file_name == 'acc.out')):\n",
    "            for i in range(len(data[file_name])):\n",
    "                data[file_name][i] = data[file_name][i]*100\n",
    "        \n",
    "        # mean computed over tasks\n",
    "        mean_over_tasks[file_name] = np.mean(data[file_name], axis=1)\n",
    "\n",
    "        # now compute the mean of forgetting over the class subcategories\n",
    "        \n",
    "        classes_forg_subcat_mean = []\n",
    "\n",
    "        \n",
    "        for name_class in data_dict:\n",
    "            class_means_all_seeds = []\n",
    "            for idx_seed in seeds:\n",
    "                class_indices = [subcategories_names.index(subcat) for subcat in data_dict[name_class]]\n",
    "                current_subcat_values = [mean_over_tasks[file_name][idx_seed][idx] for idx in class_indices]\n",
    "                class_means_all_seeds.append(np.mean(current_subcat_values, axis=0))\n",
    "            classes_forg_subcat_mean.append(class_means_all_seeds)\n",
    "        \n",
    "        # mean and std computed over the seeds\n",
    "        subcat_means_over_seeds = np.mean(classes_forg_subcat_mean, axis=1)\n",
    "        subcat_std_over_seeds = np.std(classes_forg_subcat_mean, axis=1, ddof=1)\n",
    "            \n",
    "        np.savetxt(os.path.join(mean_over_tasks_path,file_name), np.column_stack(subcat_means_over_seeds),delimiter=',',fmt='%.3f')\n",
    "        np.savetxt(os.path.join(std_over_tasks_path,file_name), np.column_stack(subcat_std_over_seeds),delimiter=',',fmt='%.3f')\n",
    "        \n",
    "        tmp_string_array = []\n",
    "        for i in range(len(subcat_means_over_seeds)):\n",
    "            \n",
    "            tmp_string = \"{:.1f}\".format(subcat_means_over_seeds[i])+'\\u00B1'+\"{:.1f}\".format(subcat_std_over_seeds[i])\n",
    "            tmp_string_array.append(tmp_string)\n",
    "            np.savetxt(os.path.join(mean_std_over_tasks_path,file_name), np.column_stack(tmp_string_array),delimiter=',',fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_incdec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
