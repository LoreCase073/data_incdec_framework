{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports to work with...\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from cl_framework.continual_learning.metrics.metric_evaluator_incdec import MetricEvaluatorIncDec\n",
    "from cl_framework.utilities.matrix_logger import IncDecLogger\n",
    "from torchmetrics import Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_path = '../runs_trainings/freeze_backbone/baseline_multiclass/weighted'\n",
    "\"\"\" results_path = [] \"\"\"\n",
    "\n",
    "\"\"\" results_path =  \"\"\"\n",
    "results_path = ['../runs_trainings/no_freeze/multilabel/weighted',\n",
    "                '../runs_trainings/no_freeze/decremental_multilabel/weighted',         \n",
    "                '../runs_trainings/no_freeze/incremental_decremental_multilabel/weighted',\n",
    "                '../runs_trainings/no_freeze/joint_incremental_multilabel/weighted',\n",
    "                '../runs_trainings/no_freeze/joint_incremental_restored_multilabel/weighted/reset',\n",
    "\n",
    "                '../runs_trainings/freeze_backbone/joint_incremental_restored_multilabel/weighted/reset',\n",
    "                '../runs_trainings/freeze_backbone/joint_incremental_multilabel/weighted/new',\n",
    "                '../runs_trainings/freeze_backbone/incremental_decremental_multilabel/weighted/new',\n",
    "                '../runs_trainings/freeze_backbone/decremental_multilabel/weighted/new',\n",
    "                '../runs_trainings/freeze_backbone/baseline_multilabel/weighted/new',]\n",
    "seeds = [0,1,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars_file_names = ['acc.out', 'map_weighted.out','mean_ap.out']\n",
    "forg_scalars_file_names = ['forg_acc.out','forg_mean_ap.out']\n",
    "multidim_file_names = ['acc_per_class.out','ap.out',\n",
    "                       'precision_per_class.out','recall_per_class.out']\n",
    "forg_multidim_file_names = ['forg_accuracy_per_class.out','forg_accuracy_per_subcategory.out',\n",
    "                       'forg_ap_per_subcategory.out','forg_ap.out','forg_recall_per_class.out',\n",
    "                       'forg_recall_per_subcategory.out','forg_precision_per_class.out','forg_precision_per_subcategory.out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res_path in results_path:\n",
    "    statistics_save_path = os.path.join(res_path,'statistics')\n",
    "    if not os.path.exists(statistics_save_path):\n",
    "        os.mkdir(statistics_save_path)\n",
    "    mean_std_over_tasks_path = os.path.join(statistics_save_path,'mean_std_over_tasks')\n",
    "    if not os.path.exists(mean_std_over_tasks_path):\n",
    "        os.mkdir(mean_std_over_tasks_path)\n",
    "    \n",
    "    data = {}\n",
    "    for file_name in scalars_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data)\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    mean_over_tasks = {}\n",
    "    for file_name in scalars_file_names:\n",
    "        mean_over_tasks[file_name] = np.mean(data[file_name], axis=1)\n",
    "\n",
    "        data_mean[file_name] = np.mean(mean_over_tasks[file_name], axis=0)\n",
    "        data_std[file_name] = np.std(mean_over_tasks[file_name], axis=0, ddof=1)\n",
    "        tmp_string = [\"{:.1f}\".format(data_mean[file_name])+'\\u00B1'+\"{:.1f}\".format(data_std[file_name])]\n",
    "        np.savetxt(os.path.join(mean_std_over_tasks_path,file_name), tmp_string,delimiter=',',fmt='%s')\n",
    "\n",
    "    data = {}\n",
    "    for file_name in forg_scalars_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data[1:])\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    mean_over_tasks = {}\n",
    "    for file_name in forg_scalars_file_names:\n",
    "        mean_over_tasks[file_name] = np.mean(data[file_name], axis=1)\n",
    "\n",
    "        data_mean[file_name] = np.mean(mean_over_tasks[file_name], axis=0)\n",
    "        data_std[file_name] = np.std(mean_over_tasks[file_name], axis=0, ddof=1)\n",
    "        tmp_string = [\"{:.1f}\".format(data_mean[file_name])+'\\u00B1'+\"{:.1f}\".format(data_std[file_name])]\n",
    "        np.savetxt(os.path.join(mean_std_over_tasks_path,file_name), tmp_string,delimiter=',',fmt='%s')\n",
    "    # Multidim metrics\n",
    "    data = {}\n",
    "    for file_name in multidim_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data)\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    mean_over_tasks = {}\n",
    "    for file_name in multidim_file_names:\n",
    "        # this is done because i have the ap in 0,1 range, want to be in percentage\n",
    "        if not ((file_name == 'mean_ap.out') or (file_name == 'map_weighted.out') or (file_name == 'forg_mean_ap.out') or (file_name == 'forg_acc.out') or (file_name == 'acc.out')):\n",
    "            for i in range(len(data[file_name])):\n",
    "                data[file_name][i] = data[file_name][i]*100\n",
    "\n",
    "        mean_over_tasks[file_name] = np.mean(data[file_name], axis=1)\n",
    "        \n",
    "        data_mean[file_name] = np.mean(mean_over_tasks[file_name], axis=0)\n",
    "        data_std[file_name] = np.std(mean_over_tasks[file_name], axis=0, ddof=1)\n",
    "\n",
    "        \n",
    "        tmp_mean = data_mean[file_name].tolist()\n",
    "        tmp_std = data_std[file_name].tolist()\n",
    "        tmp_string_array = []\n",
    "        for i in range(len(tmp_mean)):\n",
    "            \n",
    "            tmp_string = \"{:.1f}\".format(tmp_mean[i])+'\\u00B1'+\"{:.1f}\".format(tmp_std[i])\n",
    "            tmp_string_array.append(tmp_string)\n",
    "            np.savetxt(os.path.join(mean_std_over_tasks_path,file_name), np.column_stack(tmp_string_array),delimiter=',',fmt='%s')\n",
    "    \n",
    "    data = {}\n",
    "    for file_name in forg_multidim_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data[1:])\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    mean_over_tasks = {}\n",
    "    for file_name in forg_multidim_file_names:\n",
    "        # this is done because i have the ap in 0,1 range, want to be in percentage\n",
    "        if not ((file_name == 'mean_ap.out') or (file_name == 'map_weighted.out') or (file_name == 'forg_mean_ap.out') or (file_name == 'forg_acc.out') or (file_name == 'acc.out')):\n",
    "            for i in range(len(data[file_name])):\n",
    "                data[file_name][i] = data[file_name][i]*100\n",
    "        mean_over_tasks[file_name] = np.mean(data[file_name], axis=1)\n",
    "        \n",
    "        data_mean[file_name] = np.mean(mean_over_tasks[file_name], axis=0)\n",
    "        data_std[file_name] = np.std(mean_over_tasks[file_name], axis=0, ddof=1)\n",
    "\n",
    "        \n",
    "        tmp_mean = data_mean[file_name].tolist()\n",
    "        tmp_std = data_std[file_name].tolist()\n",
    "        tmp_string_array = []\n",
    "        for i in range(len(tmp_mean)):\n",
    "            \n",
    "            tmp_string = \"{:.1f}\".format(tmp_mean[i])+'\\u00B1'+\"{:.1f}\".format(tmp_std[i])\n",
    "            tmp_string_array.append(tmp_string)\n",
    "            np.savetxt(os.path.join(mean_std_over_tasks_path,file_name), np.column_stack(tmp_string_array),delimiter=',',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving only the mean to create the bar plots\n",
    "for res_path in results_path:\n",
    "    statistics_save_path = os.path.join(res_path,'statistics')\n",
    "    if not os.path.exists(statistics_save_path):\n",
    "        os.mkdir(statistics_save_path)\n",
    "    mean_over_tasks_path = os.path.join(statistics_save_path,'mean_over_tasks')\n",
    "    if not os.path.exists(mean_over_tasks_path):\n",
    "        os.mkdir(mean_over_tasks_path)\n",
    "    std_over_tasks_path = os.path.join(statistics_save_path,'std_over_tasks')\n",
    "    if not os.path.exists(std_over_tasks_path):\n",
    "        os.mkdir(std_over_tasks_path)\n",
    "    \n",
    "    data = {}\n",
    "    for file_name in scalars_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data)\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    mean_over_tasks = {}\n",
    "    for file_name in scalars_file_names:\n",
    "        mean_over_tasks[file_name] = np.mean(data[file_name], axis=1)\n",
    "\n",
    "        data_mean[file_name] = [np.mean(mean_over_tasks[file_name], axis=0)]\n",
    "        data_std[file_name] = [np.std(mean_over_tasks[file_name], axis=0, ddof=1)]\n",
    "        np.savetxt(os.path.join(mean_over_tasks_path,file_name), data_mean[file_name],delimiter=',',fmt='%.3f')\n",
    "        np.savetxt(os.path.join(std_over_tasks_path,file_name), data_std[file_name],delimiter=',',fmt='%.3f')\n",
    "\n",
    "    data = {}\n",
    "    for file_name in forg_scalars_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data[1:])\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    mean_over_tasks = {}\n",
    "    for file_name in forg_scalars_file_names:\n",
    "        mean_over_tasks[file_name] = np.mean(data[file_name], axis=1)\n",
    "\n",
    "        data_mean[file_name] = [np.mean(mean_over_tasks[file_name], axis=0)]\n",
    "        \n",
    "        np.savetxt(os.path.join(mean_over_tasks_path,file_name), data_mean[file_name],delimiter=',',fmt='%.3f')\n",
    "        data_std[file_name] = [np.std(mean_over_tasks[file_name], axis=0, ddof=1)]\n",
    "        np.savetxt(os.path.join(std_over_tasks_path,file_name), data_std[file_name],delimiter=',',fmt='%.3f')\n",
    "\n",
    "    data = {}\n",
    "    for file_name in multidim_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data)\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    mean_over_tasks = {}\n",
    "    for file_name in multidim_file_names:\n",
    "        # this is done because i have the ap in 0,1 range, want to be in percentage\n",
    "        if not ((file_name == 'mean_ap.out') or (file_name == 'map_weighted.out') or (file_name == 'forg_mean_ap.out') or (file_name == 'forg_acc.out') or (file_name == 'acc.out')):\n",
    "            for i in range(len(data[file_name])):\n",
    "                data[file_name][i] = data[file_name][i]*100\n",
    "        mean_over_tasks[file_name] = np.mean(data[file_name], axis=1)\n",
    "        \n",
    "        data_mean[file_name] = np.mean(mean_over_tasks[file_name], axis=0)\n",
    "        data_std[file_name] = np.std(mean_over_tasks[file_name], axis=0, ddof=1)\n",
    "\n",
    "        \n",
    "        tmp_mean = data_mean[file_name].tolist()\n",
    "        \n",
    "        np.savetxt(os.path.join(mean_over_tasks_path,file_name), np.column_stack(tmp_mean),delimiter=',',fmt='%.3f')\n",
    "\n",
    "        tmp_std = data_std[file_name].tolist()\n",
    "\n",
    "        np.savetxt(os.path.join(std_over_tasks_path,file_name), np.column_stack(tmp_std),delimiter=',',fmt='%.3f')\n",
    "        \n",
    "    \n",
    "    data = {}\n",
    "    for file_name in forg_multidim_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data[1:])\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    mean_over_tasks = {}\n",
    "    for file_name in forg_multidim_file_names:\n",
    "        # this is done because i have the ap in 0,1 range, want to be in percentage\n",
    "        if not ((file_name == 'mean_ap.out') or (file_name == 'map_weighted.out') or (file_name == 'forg_mean_ap.out') or (file_name == 'forg_acc.out') or (file_name == 'acc.out')):\n",
    "            for i in range(len(data[file_name])):\n",
    "                data[file_name][i] = data[file_name][i]*100\n",
    "        mean_over_tasks[file_name] = np.mean(data[file_name], axis=1)\n",
    "        \n",
    "        data_mean[file_name] = np.mean(mean_over_tasks[file_name], axis=0)\n",
    "\n",
    "        data_std[file_name] = np.std(mean_over_tasks[file_name], axis=0, ddof=1)\n",
    "\n",
    "        \n",
    "        tmp_mean = data_mean[file_name].tolist()\n",
    "        \n",
    "        np.savetxt(os.path.join(mean_over_tasks_path,file_name), np.column_stack(tmp_mean),delimiter=',',fmt='%.3f')\n",
    "\n",
    "        tmp_std = data_std[file_name].tolist()\n",
    "\n",
    "        np.savetxt(os.path.join(std_over_tasks_path,file_name), np.column_stack(tmp_std),delimiter=',',fmt='%.3f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_incdec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
