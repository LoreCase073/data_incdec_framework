{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports to work with...\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from cl_framework.continual_learning.metrics.metric_evaluator_incdec import MetricEvaluatorIncDec\n",
    "from cl_framework.utilities.matrix_logger import IncDecLogger\n",
    "from torchmetrics import Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" results_path = ['../runs_trainings/no_freeze/multilabel/weighted',\n",
    "                '../runs_trainings/no_freeze/decremental_multilabel/weighted',         \n",
    "                '../runs_trainings/no_freeze/incremental_decremental_multilabel/weighted',\n",
    "                '../runs_trainings/no_freeze/joint_incremental_multilabel/weighted',\n",
    "                '../runs_trainings/no_freeze/joint_incremental_restored_multilabel/weighted/reset',\n",
    "\n",
    "                '../runs_trainings/freeze_backbone/joint_incremental_restored_multilabel/weighted/reset',\n",
    "                '../runs_trainings/freeze_backbone/joint_incremental_multilabel/weighted/new',\n",
    "                '../runs_trainings/freeze_backbone/incremental_decremental_multilabel/weighted/new',\n",
    "                '../runs_trainings/freeze_backbone/decremental_multilabel/weighted/new',\n",
    "                '../runs_trainings/freeze_backbone/baseline_multilabel/weighted/new',\n",
    "\n",
    "                '../runs_trainings/fd/incremental_decremental/lambda_1',\n",
    "                '../runs_trainings/fd/incremental_decremental/lambda_01',\n",
    "                '../runs_trainings/fd/incremental_decremental/lambda_10',\n",
    "                '../runs_trainings/fd/decremental/lambda_01',\n",
    "                '../runs_trainings/fd/joint_incremental/lambda_01',\n",
    "                '../runs_trainings/fd/baseline/lambda_01',\n",
    "\n",
    "\n",
    "                '../runs_trainings/lwf/incremental_decremental/lambda_10/temp_1',\n",
    "                '../runs_trainings/lwf/incremental_decremental/lambda_2/temp_1',\n",
    "                '../runs_trainings/lwf/incremental_decremental/lambda_1/temp_1',\n",
    "                '../runs_trainings/lwf/baseline/lambda_1',\n",
    "                '../runs_trainings/lwf/decremental/lambda_1',\n",
    "                '../runs_trainings/lwf/joint_incremental/lambda_1',\n",
    "                ] \"\"\"\n",
    "results_path = ['../runs_trainings/from_checkpoint_sgd/no_freeze/incremental_decremental/4_4_lr',\n",
    "                '../runs_trainings/from_checkpoint_sgd/no_freeze/joint_incremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/no_freeze/decremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/no_freeze/data_substitution',\n",
    "\n",
    "                '../runs_trainings/from_checkpoint_sgd/freeze/incremental_decremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/freeze/joint_incremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/freeze/decremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/freeze/data_substitution',\n",
    "\n",
    "                '../runs_trainings/from_checkpoint_sgd/fd/incremental_decremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/fd/joint_incremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/fd/decremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/fd/data_substitution',\n",
    "\n",
    "                '../runs_trainings/from_checkpoint_sgd/lwf/incremental_decremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/lwf/joint_incremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/lwf/decremental',\n",
    "                '../runs_trainings/from_checkpoint_sgd/lwf/data_substitution',\n",
    "\n",
    "                ]\n",
    "seeds = [0,1,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars_file_names = ['acc.out', 'forg_acc.out','forg_mean_ap.out','map_weighted.out','mean_ap.out']\n",
    "multidim_file_names = ['acc_per_class.out','ap.out','forg_accuracy_per_class.out','forg_accuracy_per_subcategory.out',\n",
    "                       'forg_ap_per_subcategory.out','forg_ap.out','forg_recall_per_class.out','forg_precision_per_class.out','forg_recall_per_subcategory.out',\n",
    "                       'forg_precision_per_subcategory.out','precision_per_class.out','recall_per_class.out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res_path in results_path:\n",
    "    statistics_save_path = os.path.join(res_path,'statistics')\n",
    "    if not os.path.exists(statistics_save_path):\n",
    "        os.mkdir(statistics_save_path)\n",
    "    mean_std_save_path = os.path.join(statistics_save_path,'mean_std_from_seeds')\n",
    "    if not os.path.exists(mean_std_save_path):\n",
    "        os.mkdir(mean_std_save_path)\n",
    "    mean_save_path = os.path.join(statistics_save_path,'mean_from_seeds')\n",
    "    if not os.path.exists(mean_save_path):\n",
    "        os.mkdir(mean_save_path)\n",
    "    std_save_path = os.path.join(statistics_save_path,'std_from_seeds')\n",
    "    if not os.path.exists(std_save_path):\n",
    "        os.mkdir(std_save_path)\n",
    "\n",
    "    data = {}\n",
    "    for file_name in scalars_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data)\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    for file_name in scalars_file_names:\n",
    "        # this is done because i have the ap in 0,1 range, want to be in percentage\n",
    "        if not ((file_name == 'mean_ap.out') or (file_name == 'map_weighted.out') or (file_name == 'forg_mean_ap.out') or (file_name == 'forg_acc.out') or (file_name == 'acc.out')):\n",
    "            for i in range(len(data[file_name])):\n",
    "                data[file_name][i] = data[file_name][i]*100\n",
    "        data_mean[file_name] = np.mean(data[file_name], axis=0)\n",
    "        data_std[file_name] = np.std(data[file_name], axis=0, ddof=1)\n",
    "\n",
    "        \n",
    "        np.savetxt(os.path.join(mean_save_path,file_name), data_mean[file_name],delimiter=',',fmt='%.1f')\n",
    "        np.savetxt(os.path.join(std_save_path,file_name), data_std[file_name],delimiter=',',fmt='%.1f')\n",
    "        \n",
    "        tmp_mean = data_mean[file_name].tolist()\n",
    "        tmp_std = data_std[file_name].tolist()\n",
    "        tmp_string_list = []\n",
    "        for idx in range(len(tmp_mean)):\n",
    "            tmp_string = \"{:.1f}\".format(tmp_mean[idx])+'\\u00B1'+\"{:.1f}\".format(tmp_std[idx])\n",
    "            tmp_string_list.append(tmp_string)\n",
    "        data_string[file_name] = tmp_string_list\n",
    "        np.savetxt(os.path.join(mean_std_save_path,file_name), data_string[file_name],delimiter=',',fmt='%s')\n",
    "\n",
    "    data = {}\n",
    "    for file_name in multidim_file_names:\n",
    "        data[file_name] = []\n",
    "        for idx_seed in seeds:\n",
    "            seed_path = os.path.join(res_path, 'seed_' + str(idx_seed))\n",
    "            \n",
    "            for exp_dir in os.listdir(seed_path):\n",
    "                exp_path = os.path.join(seed_path,exp_dir)\n",
    "                logger_path = os.path.join(exp_path,'logger')\n",
    "                file_path = os.path.join(logger_path,file_name)\n",
    "                file_data = np.loadtxt(file_path,delimiter=',')\n",
    "                data[file_name].append(file_data)\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    data_string = {}\n",
    "    for file_name in multidim_file_names:\n",
    "        # this is done because i have the ap in 0,1 range, want to be in percentage\n",
    "        if not ((file_name == 'mean_ap.out') or (file_name == 'map_weighted.out') or (file_name == 'forg_mean_ap.out') or (file_name == 'forg_acc.out') or (file_name == 'acc.out')):\n",
    "            for i in range(len(data[file_name])):\n",
    "                data[file_name][i] = data[file_name][i]*100\n",
    "        data_mean[file_name] = np.mean(data[file_name], axis=0)\n",
    "        data_std[file_name] = np.std(data[file_name], axis=0, ddof=1)\n",
    "\n",
    "        np.savetxt(os.path.join(mean_save_path,file_name), data_mean[file_name],delimiter=',',fmt='%.1f')\n",
    "        np.savetxt(os.path.join(std_save_path,file_name), data_std[file_name],delimiter=',',fmt='%.1f')\n",
    "        \n",
    "        tmp_mean = data_mean[file_name].tolist()\n",
    "        tmp_std = data_std[file_name].tolist()\n",
    "        tmp_string_array = []\n",
    "        for i in range(len(tmp_mean)):\n",
    "            tmp_string_list = []\n",
    "            for j in range(len(tmp_mean[i])):\n",
    "                tmp_string = \"{:.1f}\".format(tmp_mean[i][j])+'\\u00B1'+\"{:.1f}\".format(tmp_std[i][j])\n",
    "                tmp_string_list.append(tmp_string)\n",
    "            tmp_string_array.append(tmp_string_list)\n",
    "            data_string[file_name] = tmp_string_array\n",
    "            np.savetxt(os.path.join(mean_std_save_path,file_name), data_string[file_name],delimiter=',',fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_incdec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
